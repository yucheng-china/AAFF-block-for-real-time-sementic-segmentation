{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:469: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Administrator\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:470: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Administrator\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:471: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Administrator\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:472: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Administrator\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:473: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Administrator\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:476: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing the model ...\n",
      "This model has 2674832 trainable parameters\n",
      "Loading the data ...\n",
      "\n",
      "***** Begin training *****\n",
      "Dataset --> CamVid\n",
      "Model --> BiSeNet\n",
      "Crop Height --> 224\n",
      "Crop Width --> 224\n",
      "Num Epochs --> 2\n",
      "Batch Size --> 1\n",
      "Num Classes --> 32\n",
      "Data Augmentation:\n",
      "\tVertical Flip --> True\n",
      "\tHorizontal Flip --> True\n",
      "\tBrightness Alteration --> 0.1\n",
      "\tRotation --> 30\n",
      "\n",
      "[2020-08-26 23:50:47] Epoch = 0 Count = 20 Current_Loss = 2.1063 Time = 22.04\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os,time,cv2, sys, math\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import numpy as np\n",
    "import time, datetime\n",
    "import argparse\n",
    "import random\n",
    "import subprocess\n",
    "import configuration\n",
    "from utils import utils, helpers\n",
    "from builders import model_builder\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "# specify the GPU \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '/device:GPU:0'\n",
    "# import the train configuration\n",
    "train_config = configuration.train_config\n",
    "\n",
    "# use 'Agg' on matplotlib so that plots could be generated even without Xserver\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "# define loolean type for data augmentation\n",
    "def str2bool(v):\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')\n",
    "\n",
    "# specify the training hyper-parameters\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--num_epochs', type=int, default=62, help='Number of epochs to train for')\n",
    "parser.add_argument('--epoch_start_i', type=int, default=0, help='Start counting epochs from this number')\n",
    "parser.add_argument('--checkpoint_step', type=int, default=4, help='How often to save checkpoints (epochs)')\n",
    "parser.add_argument('--validation_step', type=int, default=1, help='How often to perform validation (epochs)')\n",
    "parser.add_argument('--image', type=str, default=None, help='The image you want to predict on. Only valid in \"predict\" mode.')\n",
    "parser.add_argument('--dataset', type=str, default=\"CamVid\", help='Dataset you are using.')\n",
    "parser.add_argument('--crop_height', type=int, default=640, help='Height of cropped input image to network')\n",
    "parser.add_argument('--crop_width', type=int, default=800, help='Width of cropped input image to network')\n",
    "parser.add_argument('--batch_size', type=int, default=1, help='Number of images in each batch')\n",
    "parser.add_argument('--num_val_images', type=int, default=30, help='The number of images to used for validations')\n",
    "parser.add_argument('--h_flip', type=str2bool, default=True, help='Whether to randomly flip the image horizontally for data augmentation')\n",
    "parser.add_argument('--v_flip', type=str2bool, default=True, help='Whether to randomly flip the image vertically for data augmentation')\n",
    "parser.add_argument('--brightness', type=float, default=0.1, help='Whether to randomly change the image brightness for data augmentation. Specifies the max bightness change as a factor between 0.0 and 1.0. For example, 0.1 represents a max brightness change of 10%% (+-).')\n",
    "parser.add_argument('--rotation', type=float, default=30, help='Whether to randomly rotate the image for data augmentation. Specifies the max rotation angle in degrees.')\n",
    "parser.add_argument('--model', type=str, default=\"BiSeNet\", help='The model you are using. See model_builder.py for supported models')\n",
    "parser.add_argument('--frontend', type=str, default=\"xception\", help='The frontend you are using. See frontend_builder.py for supported models')\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "# define the data augmentation method\n",
    "def data_augmentation(input_image, output_image):\n",
    "    # crop the input image to the specified size\n",
    "    input_image, output_image = utils.random_crop(input_image, output_image, args.crop_height, args.crop_width)\n",
    "    # 0: filp vertical  1: flip horizontal  \n",
    "    if args.h_flip and random.randint(0,1):\n",
    "        input_image = cv2.flip(input_image, 1)\n",
    "        output_image = cv2.flip(output_image, 1)\n",
    "    if args.v_flip and random.randint(0,1):\n",
    "        input_image = cv2.flip(input_image, 0)\n",
    "        output_image = cv2.flip(output_image, 0)\n",
    "    # random change the brightness\n",
    "    if args.brightness:\n",
    "        factor = 1.0 + random.uniform(-1.0*args.brightness, args.brightness)\n",
    "        table = np.array([((i / 255.0) * factor) * 255 for i in np.arange(0, 256)]).astype(np.uint8)\n",
    "        input_image = cv2.LUT(input_image, table)\n",
    "    # random rotation for the specified degree, etc. 30.\n",
    "    if args.rotation:\n",
    "        angle = random.uniform(-1*args.rotation, args.rotation)\n",
    "    if args.rotation:\n",
    "        M = cv2.getRotationMatrix2D((input_image.shape[1]//2, input_image.shape[0]//2), angle, 1.0)\n",
    "        input_image = cv2.warpAffine(input_image, M, (input_image.shape[1], input_image.shape[0]), flags=cv2.INTER_NEAREST)\n",
    "        output_image = cv2.warpAffine(output_image, M, (output_image.shape[1], output_image.shape[0]), flags=cv2.INTER_NEAREST)\n",
    "    # return the pre-processed image.\n",
    "    return input_image, output_image\n",
    "\n",
    "# Global_step is used as a count in training, adding 1 for each batch of training\n",
    "global_step = tf.Variable(initial_value=0,\n",
    "                           name='global_step',\n",
    "                           trainable=False,\n",
    "                           collections=[tf.GraphKeys.GLOBAL_STEP, tf.GraphKeys.GLOBAL_VARIABLES])\n",
    "\n",
    "# define the configuration of learning rate\n",
    "def _configure_learning_rate(train_config, global_step):\n",
    "    lr_config = train_config['lr_config']\n",
    "    # specify the iterations for an epoch\n",
    "    num_batches_per_epoch = int(421 / args.batch_size)\n",
    "    # apply the 'polynomial' learning rate policy\n",
    "    T_total = (int(num_batches_per_epoch)+1) * args.num_epochs\n",
    "    return lr_config['initial_lr'] * (1 - tf.to_float(global_step)/T_total)**lr_config['power']\n",
    "\n",
    "\n",
    "# Get the names of the classes so we can record the evaluation results\n",
    "class_names_list, label_values = helpers.get_label_info(os.path.join(args.dataset, \"class_dict.csv\"))\n",
    "class_names_string = \"\"\n",
    "for class_name in class_names_list:\n",
    "    if not class_name == class_names_list[-1]:\n",
    "        class_names_string = class_names_string + class_name + \", \"\n",
    "    else:\n",
    "        class_names_string = class_names_string + class_name\n",
    "# there are 32 classes in this project\n",
    "num_classes = len(label_values)\n",
    "# allow use gpu for training\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess=tf.Session(config=config)\n",
    "\n",
    "# net_input is the RGB image with 3 channels, net_output is the semantic image with 32 channels\n",
    "net_input = tf.placeholder(tf.float32,shape=[None,None,None,3])\n",
    "net_output = tf.placeholder(tf.float32,shape=[None,None,None,num_classes])\n",
    "# load the network\n",
    "network, init_fn = model_builder.build_model(model_name=args.model, frontend=args.frontend, net_input=net_input, num_classes=num_classes, crop_width=args.crop_width, \n",
    "crop_height=args.crop_height, is_training=True)\n",
    "# Compute your softmax cross entropy loss\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=network, labels=net_output))\n",
    "# define learning rate configuration\n",
    "learning_rate = _configure_learning_rate(train_config, global_step)\n",
    "# define the configuration of optimizer\n",
    "optimizer_config = train_config['optimizer_config']\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate,\n",
    "                                        momentum=optimizer_config['momentum'],\n",
    "                                        use_nesterov=optimizer_config['use_nesterov'],\n",
    "                                        name='Momentum')\n",
    "# minimize the loss\n",
    "opt = optimizer.minimize(loss, var_list=[var for var in tf.trainable_variables()])\n",
    "# define a saver instance\n",
    "saver=tf.train.Saver(max_to_keep=500)\n",
    "# Initialize the parameters of the model\n",
    "sess.run(tf.global_variables_initializer())\n",
    "# Count total number of parameters in the model\n",
    "utils.count_params()\n",
    "\n",
    "\n",
    "# If a pre-trained ResNet is required, load the weights.\n",
    "# This must be done AFTER the variables are initialized with sess.run(tf.global_variables_initializer())\n",
    "if init_fn is not None:\n",
    "    init_fn(sess)\n",
    "    \n",
    "# Load a previous checkpoint if desired\n",
    "model_checkpoint_name = \"checkpoints/latest_model_\" + args.model + \"_\" + args.dataset + \".ckpt\"\n",
    "\n",
    "# Load the data\n",
    "print(\"Loading the data ...\")\n",
    "train_input_names,train_output_names, val_input_names, val_output_names, test_input_names, test_output_names = utils.prepare_data(dataset_dir=args.dataset)\n",
    "# firstly, show the training hyper-parameters\n",
    "print(\"\\n***** Begin training *****\")\n",
    "print(\"Dataset -->\", args.dataset)\n",
    "print(\"Model -->\", args.model)\n",
    "print(\"Crop Height -->\", args.crop_height)\n",
    "print(\"Crop Width -->\", args.crop_width)\n",
    "print(\"Num Epochs -->\", args.num_epochs)\n",
    "print(\"Batch Size -->\", args.batch_size)\n",
    "print(\"Num Classes -->\", num_classes)\n",
    "print(\"Data Augmentation:\")\n",
    "print(\"\\tVertical Flip -->\", args.v_flip)\n",
    "print(\"\\tHorizontal Flip -->\", args.h_flip)\n",
    "print(\"\\tBrightness Alteration -->\", args.brightness)\n",
    "print(\"\\tRotation -->\", args.rotation)\n",
    "print(\"\")\n",
    "\n",
    "# create some empty arrays for further storage\n",
    "avg_loss_per_epoch = []\n",
    "avg_scores_per_epoch = []\n",
    "avg_iou_per_epoch = []\n",
    "val_indices = []\n",
    "# specify the numbers of validation image for an epoch\n",
    "num_vals = min(args.num_val_images, len(val_input_names))\n",
    "\n",
    "# Set random seed to make sure models are validated on the same validation images.\n",
    "# so that the validation results for different networks can be compared\n",
    "random.seed(16)\n",
    "val_indices=random.sample(range(0,len(val_input_names)),num_vals)\n",
    "\n",
    "# start the training process\n",
    "for epoch in range(args.epoch_start_i, args.num_epochs):\n",
    "    current_losses = []\n",
    "    count=0\n",
    "\n",
    "    # random sort the training images\n",
    "    id_list = np.random.permutation(len(train_input_names))\n",
    "    # compute the number of iterations for an epoch\n",
    "    num_iters = int(np.floor(len(id_list) / args.batch_size))\n",
    "    # time start point\n",
    "    st = time.time()\n",
    "    epoch_st=time.time()\n",
    "    for i in range(num_iters):\n",
    "        input_image_batch = []\n",
    "        output_image_batch = []\n",
    "        # Collect a batch of images\n",
    "        for j in range(args.batch_size):\n",
    "            index = i*args.batch_size + j\n",
    "            id = id_list[index]\n",
    "            input_image = utils.load_image(train_input_names[id])\n",
    "            output_image = utils.load_image(train_output_names[id])\n",
    "\n",
    "            with tf.device('/gpu:0'):\n",
    "                input_image, output_image = data_augmentation(input_image, output_image)\n",
    "\n",
    "                # Prep the data. Make sure the labels are in one-hot format\n",
    "                input_image = np.float32(input_image) / 255.0\n",
    "                output_image = np.float32(helpers.one_hot_it(label=output_image, label_values=label_values))\n",
    "                input_image_batch.append(np.expand_dims(input_image, axis=0))\n",
    "                output_image_batch.append(np.expand_dims(output_image, axis=0))\n",
    "\n",
    "        if args.batch_size == 1:\n",
    "            input_image_batch = input_image_batch[0]\n",
    "            output_image_batch = output_image_batch[0]\n",
    "        else:\n",
    "            input_image_batch = np.squeeze(np.stack(input_image_batch, axis=1))\n",
    "            output_image_batch = np.squeeze(np.stack(output_image_batch, axis=1))\n",
    "            \n",
    "        # Do the training, opt is the optimizer, loss is cross-entropy, net_input is the input image, net_output is the labels\n",
    "        _,current=sess.run([opt,loss],feed_dict={net_input:input_image_batch,net_output:output_image_batch})\n",
    "        # storage the current loss\n",
    "        current_losses.append(current)\n",
    "        count = count + args.batch_size\n",
    "        # show the training details for every 20 input images, epoch, count and current loss\n",
    "        if count % 20 == 0:\n",
    "            string_print = \"Epoch = %d Count = %d Current_Loss = %.4f Time = %.2f\"%(epoch,count,current,time.time()-st)\n",
    "            utils.LOG(string_print)\n",
    "            st = time.time()\n",
    "    # show the average loss for a whole process of validation\n",
    "    mean_loss = np.mean(current_losses)\n",
    "    avg_loss_per_epoch.append(mean_loss)\n",
    "\n",
    "    # Create folder if needed, there are four files in the 'checkpoints' folder, storages the trained model weights\n",
    "    if not os.path.isdir(\"%s/%04d\"%(\"checkpoints\",epoch)):\n",
    "        os.makedirs(\"%s/%04d\"%(\"checkpoints\",epoch))\n",
    "\n",
    "    # Save latest checkpoint to same file name\n",
    "    print(\"Saving latest checkpoint\")\n",
    "    saver.save(sess,model_checkpoint_name)\n",
    "    # save the trained model for every (epoch/validation_step) epochs\n",
    "    if epoch % args.validation_step == 0:\n",
    "        print(\"Performing validation\")\n",
    "        target=open(\"%s/%04d/val_scores.csv\"%(\"checkpoints\",epoch),'w')\n",
    "        target.write(\"val_name, avg_accuracy, precision, recall, f1 score, mean iou, %s\\n\" % (class_names_string))\n",
    "    # create some arrays to store the training results\n",
    "        scores_list = []\n",
    "        class_scores_list = []\n",
    "        precision_list = []\n",
    "        recall_list = []\n",
    "        f1_list = []\n",
    "        iou_list = []\n",
    "\n",
    "        # Do the validation on a small set of validation images, etc.30\n",
    "        for ind in val_indices:\n",
    "            input_image = np.expand_dims(np.float32(utils.load_image(val_input_names[ind])[:args.crop_height, :args.crop_width]),axis=0)/255.0\n",
    "            gt = utils.load_image(val_output_names[ind])[:args.crop_height, :args.crop_width]\n",
    "            gt = helpers.reverse_one_hot(helpers.one_hot_it(gt, label_values))\n",
    "            # do the validation\n",
    "            output_image = sess.run(network,feed_dict={net_input:input_image})\n",
    "            output_image = np.array(output_image[0,:,:,:])\n",
    "            # convert the output image into the one-hot version\n",
    "            output_image = helpers.reverse_one_hot(output_image)\n",
    "            # revise the grey scale output image to the RCB image\n",
    "            out_vis_image = helpers.colour_code_segmentation(output_image, label_values)\n",
    "            # compute the pixel-wise accuracy, iou precise and so on\n",
    "            accuracy, class_accuracies, prec, rec, f1, iou = utils.evaluate_segmentation(pred=output_image, label=gt, num_classes=num_classes)\n",
    "            file_name = utils.filepath_to_name(val_input_names[ind])\n",
    "            # write this reults to the file\n",
    "            target.write(\"%s, %f, %f, %f, %f, %f\"%(file_name, accuracy, prec, rec, f1, iou))\n",
    "            \n",
    "            for item in class_accuracies:\n",
    "                target.write(\", %f\"%(item))\n",
    "            target.write(\"\\n\")\n",
    "            # scores_list stores the total accuracy for an output image\n",
    "            scores_list.append(accuracy)\n",
    "            # class_scores_list stores the pixel-wise classification accuracy for every objects\n",
    "            class_scores_list.append(class_accuracies)\n",
    "            precision_list.append(prec)\n",
    "            recall_list.append(rec)\n",
    "            f1_list.append(f1)\n",
    "            iou_list.append(iou)\n",
    "            # convert the grey scale ground truth image as the colored image to have a better view\n",
    "            gt = helpers.colour_code_segmentation(gt, label_values)\n",
    "            # find the file name for a specific validation image\n",
    "            file_name = os.path.basename(val_input_names[ind])\n",
    "            file_name = os.path.splitext(file_name)[0]\n",
    "            # convert it into the colored image\n",
    "            cv2.imwrite(\"%s/%04d/%s_pred.png\"%(\"checkpoints\",epoch, file_name),cv2.cvtColor(np.uint8(out_vis_image), cv2.COLOR_RGB2BGR))\n",
    "            cv2.imwrite(\"%s/%04d/%s_gt.png\"%(\"checkpoints\",epoch, file_name),cv2.cvtColor(np.uint8(gt), cv2.COLOR_RGB2BGR))\n",
    "        target.close()\n",
    "        avg_score = np.mean(scores_list)\n",
    "        class_avg_scores = np.mean(class_scores_list, axis=0)\n",
    "        avg_scores_per_epoch.append(avg_score)\n",
    "        avg_precision = np.mean(precision_list)\n",
    "        avg_recall = np.mean(recall_list)\n",
    "        avg_f1 = np.mean(f1_list)\n",
    "        avg_iou = np.mean(iou_list)\n",
    "        avg_iou_per_epoch.append(avg_iou)\n",
    "        \n",
    "        #display the average global accuracy of 30 images\n",
    "        print(\"\\nAverage validation accuracy for epoch # %04d = %f\"% (epoch, avg_score))\n",
    "        #Verify the average class accuracy of 30 images\n",
    "        print(\"Average per class validation accuracies for epoch # %04d:\"% (epoch))\n",
    "        for index, item in enumerate(class_avg_scores):\n",
    "            print(\"%s = %f\" % (class_names_list[index], item))\n",
    "        # 30 validation images average Precise\n",
    "        print(\"Validation precision = \", avg_precision)\n",
    "        # 30 validation images average recall\n",
    "        print(\"Validation recall = \", avg_recall)\n",
    "        print(\"Validation F1 score = \", avg_f1)\n",
    "        print(\"Validation IoU score = \", avg_iou)\n",
    "    # compute the time consumption for an epoch\n",
    "    epoch_time=time.time()-epoch_st\n",
    "    # compute the remaining time needed for the whole training process\n",
    "    remain_time=epoch_time*(args.num_epochs-1-epoch)\n",
    "    m, s = divmod(remain_time, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "\n",
    "    if s!=0:\n",
    "        train_time=\"Remaining training time = %d hours %d minutes %d seconds\\n\"%(h,m,s)\n",
    "    # training process completed\n",
    "    else:\n",
    "        train_time=\"Remaining training time : Training completed.\\n\"\n",
    "\n",
    "    utils.LOG(train_time)\n",
    "    scores_list = []\n",
    "    \n",
    "    # three figures will be created to show the validation accuracy, loss, mean IOU during the whole training process.\n",
    "    fig1, ax1 = plt.subplots(figsize=(11, 8))\n",
    "    ax1.plot(range(epoch+1), avg_scores_per_epoch)\n",
    "    ax1.set_title(\"Average validation accuracy vs epochs\")\n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax1.set_ylabel(\"Avg. val. accuracy\")\n",
    "\n",
    "    plt.savefig('accuracy_vs_epochs.png')\n",
    "    plt.clf()\n",
    "    fig2, ax2 = plt.subplots(figsize=(11, 8))\n",
    "\n",
    "    ax2.plot(range(epoch+1), avg_loss_per_epoch)\n",
    "    ax2.set_title(\"Average loss vs epochs\")\n",
    "    ax2.set_xlabel(\"Epoch\")\n",
    "    ax2.set_ylabel(\"Current loss\")\n",
    "\n",
    "    plt.savefig('loss_vs_epochs.png')\n",
    "    plt.clf()\n",
    "    fig3, ax3 = plt.subplots(figsize=(11, 8))\n",
    "\n",
    "    ax3.plot(range(epoch+1), avg_iou_per_epoch)\n",
    "    ax3.set_title(\"Average IoU vs epochs\")\n",
    "    ax3.set_xlabel(\"Epoch\")\n",
    "    ax3.set_ylabel(\"Current IoU\")\n",
    "    plt.savefig('iou_vs_epochs.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(tensorflow-gpu)",
   "language": "python",
   "name": "tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
